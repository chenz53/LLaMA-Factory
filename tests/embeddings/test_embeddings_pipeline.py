"""Test the embedding pipeline with multimodal embeddings."""

import logging
import unittest
from unittest.mock import MagicMock, patch

from llamafactory.data.collator import SFTDataCollatorWith4DAttentionMask
from llamafactory.data.mm_plugin import get_mm_plugin
from llamafactory.data.processor.supervised import SupervisedDataset
from llamafactory.data.template import get_template_and_fix_tokenizer
from llamafactory.extras.constants import MULTIMODAL_EMBEDDING_PLACEHOLDERS
from llamafactory.hparams import DataArguments, ModelArguments
from llamafactory.model import load_tokenizer
from llamafactory.model.model_utils.embedding_utils import get_embedding_layer_name


class TestEmbeddingsPipeline(unittest.TestCase):
    """Test the embeddings pipeline with multimodal embeddings."""

    def setUp(self):
        """Set up test fixtures."""
        self.model_args = ModelArguments(model_name_or_path="Qwen/Qwen2-1.5B-Instruct")
        self.tokenizer = load_tokenizer(self.model_args)

        # Use the first available multimodal embedding placeholder
        self.embedding_placeholder = next(iter(MULTIMODAL_EMBEDDING_PLACEHOLDERS.values()))

        # Use multimodal embedding tokens
        self.embedding_tokens = {"m1": self.embedding_placeholder}

        # Mock the embedding layer
        self.embedding_layer_name = get_embedding_layer_name(self.model_args.model_name_or_path)

        # Create a mock for the embedding layer
        self.embedding_layer = MagicMock()
        self.embedding_layer.weight.data = MagicMock()
        self.embedding_layer.weight.data.shape = [32000, 768]  # vocab_size, hidden_size

    def test_template_initialization(self):
        """Test that the template is properly initialized with multimodal embeddings."""
        template = get_template_and_fix_tokenizer(self.tokenizer, name="qwen3n")
        self.assertIsNotNone(template)

        # Mock the mm_plugin with embedding_tokens
        template.mm_plugin = get_mm_plugin(
            name="qwen3n", image_token=None, video_token=None, audio_token=None, embedding_tokens=self.embedding_tokens
        )

        # Check that embedding_tokens is properly set
        self.assertIsNotNone(template.mm_plugin.embedding_tokens)
        self.assertEqual(template.mm_plugin.embedding_tokens, self.embedding_tokens)

    def test_multimodal_embedding_count(self):
        """Test that the embedding count is correctly calculated."""
        template = get_template_and_fix_tokenizer(self.tokenizer, name="qwen3n")
        template.mm_plugin = get_mm_plugin(
            name="qwen3n", image_token=None, video_token=None, audio_token=None, embedding_tokens=self.embedding_tokens
        )

        # Create messages with multimodal embedding placeholders
        messages = [
            {"role": "user", "content": f"This is a test {self.embedding_placeholder} message."},
            {"role": "assistant", "content": "This is a response."},
        ]

        # Count embedding placeholders
        embedding_count = 0
        for message in messages:
            for placeholder in MULTIMODAL_EMBEDDING_PLACEHOLDERS.values():
                embedding_count += message["content"].count(placeholder)

        self.assertEqual(embedding_count, 1)

    def test_collator_with_multimodal_embeddings(self):
        """Test that the collator works with multimodal embeddings."""
        template = get_template_and_fix_tokenizer(self.tokenizer, name="qwen3n")
        template.mm_plugin = get_mm_plugin(
            name="qwen3n", image_token=None, video_token=None, audio_token=None, embedding_tokens=self.embedding_tokens
        )

        # Create a data collator
        data_collator = SFTDataCollatorWith4DAttentionMask(
            tokenizer=self.tokenizer,
            template=template,
            model=MagicMock(),
            use_cache=False,
            pad_to_multiple_of=8,
            block_diag_attn=False,
            attn_implementation="eager",
            compute_accuracy=False,
        )

        # Test with empty batch embeddings
        batch_embedlens = []
        self.assertEqual(sum(batch_embedlens), 0)

        # The collator should handle empty embeddings properly
        self.assertIsNotNone(data_collator)


def test_multimodal_embedding_pipeline():
    """Test the entire multimodal embedding pipeline."""

    # Mock data
    messages = [
        {"role": "user", "content": "Please analyze this <m1>"},
        {"role": "assistant", "content": "I'll analyze the embedding data."},
    ]

    # Mock embedding data using new format
    embeddings = [
        {
            "m1": [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]],
            "shape": [2, 4],
            "description": "Sample embedding data",
            "modality": "text",
        }
    ]

    print("Testing multimodal embedding pipeline...")
    print(f"Messages: {messages}")
    print(f"Embedding files: {embeddings}")

    # Process messages to count embedding placeholders
    embedding_count = 0
    for message in messages:
        for placeholder in MULTIMODAL_EMBEDDING_PLACEHOLDERS.values():
            embedding_count += message["content"].count(placeholder)

    print(f"Number of embedding placeholders: {embedding_count}")
    print(f"Number of embedding files: {len(embeddings)}")

    # Validate that counts match
    assert embedding_count == len(embeddings), (
        f"Mismatch: {embedding_count} placeholders vs {len(embeddings)} embeddings"
    )

    print("âœ“ Multimodal embedding pipeline test passed")


if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(level=logging.INFO)

    # Run the test
    test_multimodal_embedding_pipeline()

    # Run unit tests
    unittest.main()
